---
title: "Introduction to mlr3"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r, include = FALSE, warning = FALSE, message = FALSE}
# Just some preparation
knitr::opts_chunk$set(
  cache = FALSE,
  collapse = TRUE,
  comment = "#>"
)
if (require("data.table")) data.table::setDTthreads(1)
options(width=90)
set.seed(8008135)
lgr::get_logger("mlr3")$set_threshold("warn")
```

# Intro

**mlr3** is a machine learning framework for R. Together with other packages from the same developers, mostly following the naming scheme "mlr3___", it offers functionality around developing, tuning, and evaluating machine learning workflows.

We will walk through this tutorial interactively. The text is kept short to be followed in real time. There are links to the [Appendix](#appendix) for further information that you can read in your spare time.

# Prerequisites

Ensure all packages used in this tutorial are installed. This includes packages from the `mlr3` family, as well as other tools for data handling, cleaning and visualisation which we are going to use. [Appendix for more info about packages](#packages)

```{r, message=FALSE, warning=FALSE, eval = FALSE}
# install from CRAN
packages_cran = c(
    "remotes", "magrittr", "data.table", "ggplot2", "skimr", "DataExplorer",
    "OpenML", "rpart.plot", "precrec",
    "glmnet", "kknn", "MASS", "ranger", "xgboost", "e1071",
    "future", "future.apply",
    "mlr3misc", "paradox", "mlr3", "mlr3filter", "mlr3learners")
# install things from GitHub that are not yet on CRAN
packages_gith = c(
    "mlr-org/mlr3tuning", "mlr-org/mlr3pipelines", "mlr-org/mlr3viz")
to_install = setdiff(packages_cran, installed.packages()[,"Package"])
if (length(to_install)) install.packages(to_install)
lapply(packages_gith, remotes::install_github)
```

Load the packages we are going to use:

```{r, message=FALSE, warning=FALSE}
library("data.table")
library("magrittr")
library("mlr3")
library("mlr3learners")
library("ggplot2")
theme_set(theme_light())
```

# Machine Learning Use Case: German Credit Data

- The German credit data is a research data set of the University of Hamburg from 1994 donated by Prof. Hans Hoffman.
- Description (and manual download) can be found at the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).
- Goal: Classify people by their credit risk (**good** or **bad**) using 20 features.

## Importing the data

We use [OpenML](https://www.openml.org) ([R-Package](https://cran.r-project.org/package=OpenML)) to download the data set in a machine-readable format and convert it into a `data.table`:
  
```{r, message=FALSE}
# load the data set (id 31) from OpenML Library and
# convert the OpenML object to a regular data.table.
# The encapsulate() is important.
credit = mlr3misc::encapsulate("callr", function() {
  data.table::as.data.table(OpenML::getOMLDataSet(data.id = 31))
})$result
```
## Exploring the Data
- We have a look at the data set before we start modeling.
- The `str()` and `summary()` functions gives an overview of features and their type.
- The `skimr` package gives more readable summaries.
- The `DataExplorer` package lets us visualise numeric (`plot_histogram()`) and categorical (`plot_bar()`) data, as well as data relationships.
- Basic things to watch out for:
  - Skewed distributions
  - Missing values
  - Empty / rare factor variables
  - Strongly correlating features ("multicollinearity")
```{r}
str(credit)
```
```{r}
summary(credit)
```
```{r}
skimr::skim(credit)
```
```{r}
DataExplorer::plot_histogram(credit)
```
```{r}
DataExplorer::plot_bar(credit)
```
```{r}
DataExplorer::split_columns(credit) %$%
  DataExplorer::plot_correlation(continuous)
```
```{r}
DataExplorer::plot_boxplot(credit, by = "class")
```
```{r}
```
- We have empty factor levels, so we drop them:
```{r}
credit %<>% droplevels()
```

# Modeling
Considering how we are going to tackle the problem relates closely to what `mlr3` entities we will use.

- What is the problem we are trying to solve?
  - i.e. what **Task** do we use?
  - Binary classification.
  - $\Rightarrow$ We use `TaskClassif`.
- What are appropriate learning algorithms?
  - i.e. what **Learner** do we use?
  - Logistic regression, CART, Random Forest
  - $\Rightarrow$ `lrn("classif.log_reg")`, `lrn("classif.rpart")`, `lrn("classif.ranger")`
- How do we evaluate "good" performance? $\Rightarrow$ Depends on many things! Cost of false positive vs. false negative, legal requirements, ...
  - i.e. what **Measure** do we use?
  - We start with misclassification error (simle!) and will also consider AUC.
  - $\Rightarrow$ `msr("classif.ce")`, `msr("classif.auc")`

## Task Definition

- We need to consider what the name of the target variable is: `"class"`.
```{r}
task = TaskClassif$new("GermanCredit", credit, "class")
```

## Model Fitting: Logistic Regression

- Get the `Learner` from the database. It is provided by `mlr3learners` and uses R's `glm()` function.
```{r}
learner_logreg = lrn("classif.log_reg")
```
- Model fitting is easy
```{r}
learner_logreg$train(task)
```
- Inspecting the model. It is the result returned by `glm()` and can be inspected as such.
```{r}
coefficients = learner_logreg$model %>% summary() %$%
  as.data.table(coefficients, keep.rownames = TRUE)
coefficients[order(`Pr(>|z|)`)] %>% head(n = 10) %>% print(digits = 3)
```

## Model Fitting: Decision Tree

- Same procedure as before
- We fit a decision tree with depth 3 for nicer plots
```{r}
learner_cart = lrn("classif.rpart", maxdepth = 3)
learner_cart$train(task)
```
- The `rpart.plot` library gives expressive plots:
```{r}
rpart.plot::rpart.plot(learner_cart$model)
```

## Model Fitting: Random Forest

- Same procedure as before
- We let the model store the variable importance (`importance = "impurity_corrected"`) which we can inspect using `$importance()`
```{r}
learner_rf = lrn("classif.ranger", importance = "impurity")
learner_rf$train(task)
```
```{r}
learner_rf$importance()
```
- We convert the importance into a `data.table` so we can plot it
```{r}
importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)
colnames(importance) = c("Feature", "Importance")
importance
ggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() + coord_flip() + xlab("")
```

## Model Fitting: Other Models

Feel free to experiment with other `Learner`s! There are many `Learner`s defined in `mlr_learners`. You need to load the `mlr3learners` package to get access to most of them.

Use `?mlr_learners_xxx` or `help(class(lrn("xxx"))[1])` to see the help page, or use [the internet](https://mlr3learners.mlr-org.com/reference/index.html).
```{r}
mlr_learners
```

# Prediction

- A model, once trained, can be used to predict outcomes from new data.
- This new data needs to be loaded as a `Task` as well, then the `Learner`'s `$predict()` function can be used.
- We simulate new data by sampling from the credit data. No need to pay close attention what is going on here.
```{r}
newdata = as.data.table(lapply(credit, sample, size = 3, replace = TRUE))
# we need to set the 'class' column to 'NA', otherwise mlr3 thinks this is the
# actual outcome. This is desirable for performance evaluation, but not here.
newdata$class[1:3] = NA
```
```{r}
new_task = TaskClassif$new("new_credit", newdata, "class", positive = "good")
```
- Let's see what the models predict
```{r}
learner_logreg$predict(new_task)
learner_cart$predict(new_task)
learner_rf$predict(new_task)
```

- They may disagree, but which do we trust most? We should do [Performance Evaluation](#performance-evaluation) and [benchmarks](#performance-comparison-and-benchmarks)!

## Probability Prediction

- Learners may not only predict a class variable ("response"), but also their degree of "belief" / uncertainty in a given response.
- We achieve this by setting the `$predict_type` slot to `"prob"`.
- Sometimes this needs to be done *before* the learner is trained.
- Can alternatively construct with this: `lrn("classif.log_reg", predict_type = "prob")`
```{r}
learner_logreg$predict_type = "prob"
```
```{r}
learner_logreg$predict(new_task)
```
- Careful when interpreting this as probabilities!

# Performance Evaluation

## The Hard way

To show what is happening behind the scenes we will do one round of resampling ourselves
1. Decide on a train/test split.
```{r}
train_set = sample(nrow(credit), size = nrow(credit) * 2/3)
test_set = setdiff(seq_len(nrow(credit)), train_set)
head(train_set)  # rows of the dataset to use for training
```
2. Fit a model. We choose to use the `"classif.rpart"` model. The `row_ids` argument of the `$train()` function lets us train on a subset of the data.
```{r}
learner_cart$train(task, row_ids = train_set)
```
3. Make a prediction on the `test_set` rows.
```{r}
prediction = learner_cart$predict(task, row_ids = test_set)
```
```{r}
prediction
```

We now want to know how good the prediction matches reality. We can either compare `prediction$truth` and `prediction$response` manually, or use the `msr("classif.ce")` misclassification rate measure.
```{r}
# manual
mean(prediction$truth != prediction$response)
# more scalable and less errorprone: Using a msr()
prediction$score(msr("classif.ce"))
# actually, "classif.ce" is the default measure for classification:
prediction$score()
```

## The Easy Way

- What we just did was "holdout" resampling.
- `mlr3` lets us do that, and other resampling schemes, automatically.
- Get the resampling scheme as `rsmp()`:
```{r}
resampling = rsmp("holdout")
print(resampling)
```
- We use `resample()` to do the resampling calculation, and `$aggregate()` to learn the score.
```{r}
resres = resample(task, learner_logreg, resampling)
```
```{r}
resres$aggregate()
```
- The good thing is we can easily do differend kinds of resampling. E.g. repeated holdout (`"subsampling"`), or cross validation. We can also use different scores.
- Most methods do repeated train/predict cycles on different data subsets and aggregate the result (usualy as the `mean()`). Doing this manually would require us to write loops.
```{r}
resample(task, learner_logreg, rsmp("subsampling", repeats = 10))$aggregate()
```
```{r}
resample(task, learner_logreg, rsmp("cv", folds = 10))$aggregate()
```
```{r}
# false positive and false negative rate
resample(task, learner_logreg, rsmp("cv", folds = 10))$aggregate(list(
  msr("classif.fpr"),
  msr("classif.fnr")
))
```

## Performance Evaluation: Outlook

There are a few more resampling methods, and quite a few more measures. List them in
```{r}
mlr_resamplings
```
```{r}
mlr_measures
```
To get help on a resampling method, use `?mlr_resamplings_xxx`, for a measure do `?mlr_measures_xxx`. you can also use the [mlr3 reference](https://mlr3.mlr-org.com/reference/index.html) online.

Some measure, for example `"auc"`, require a "probability" prediction, instead of a response prediction, see [**Probability Prediction**](#probability-prediction).

# Performance Comparison and Benchmarks

- We could compare `Learners` by evaluating `resample()` for each of them manually.
- `benchmark()` automatically performs resampling evaluations.
- Create fully crossed designs using `benchmark_grid()`: multiple `Learner`s **x** multiple `Task`s **x** multiple `Resampling`s
```{r}
bm_design = benchmark_grid(task = task, resamplings = rsmp("cv", folds = 10),
  learners = list(
      lrn("classif.log_reg", predict_type = "prob"),
      lrn("classif.rpart", predict_type = "prob"),
      lrn("classif.ranger", predict_type = "prob")
  ))
```
- Careful, large benchmarks may take a long time! This one should take less than a minute, however.
- In General, we want use *parallelization* to speed things up on multicore machines.
```{r}
future::plan("multiprocess")
bmr = benchmark(bm_design)
```
- We can compare different measures. We compare misclassification rate and AUC.
```{r}
performances = bmr$aggregate(list(msr("classif.ce"), msr("classif.auc")))
performances[, c("learner_id", "classif.ce", "classif.auc")]
```

# Outlook

- How did we do? We can check the [OpenML](https://www.openml.org/t/31) website for performances of other machine learning methods.
  - We see `ranger` is among the top methods
- Things we have not done that should be considered:
  - We have worked with default hyperparameters, but we may want to see if tuning them helps (Day 2)
  - Some preprocessing and feature extraction steps may sometimes be helpful (Day 3)

# Appendix

## R Pro Tips

* What are the arguments of `lrn()`, `tsk()`, etc. again? -> Think about the corresponding dictionary
```{r}
mlr_learners
mlr_tasks
mlr_measures
mlr_resamplings
```

* What are the arguments of a `$new()` constructor?
```{r}
formals(TaskClassif$public_methods$initialize)
```

* What are the possible slots and functions of an object?

```{r}
# Writing `prediction$`, and pressing <TAB> should work.
# Otherwise:
names(prediction)
# try names without `()` first
# and see if it is a function
```

* How do I see the help file of an object

```{r}
# The documentation is organized by object classes
class(prediction)
# use ?PredictionClassif, ?Prediction etc.
# Try all elements listed in the class
```

## mlr3 and its Extensions

| Package | Functionality |
| :-      | :------------ |
| `mlr3`  | Framework for machine learning: `Task`, `Learner`, `resample()` and `benchmark()` |
| `mlr3learners` | Concrete `Learner`s for many popular machine learning implementations |
| `mlr3pipelines` | Dataflow programming of machine learning workflows. |
| `mlr3tuning` | Hyperparameter tuning for machine learning algorithms. |
| `mlr3filter` | Feature filtering |
| `mlr3viz` | Visualisations and plots |
| `paradox` | Auxiliary package providing (hyper)parameter handling |
| `mlr3misc` | Auxiliary functions |

## Packages

The non-`mlr3` packages we use:

| Package | Reason       |
| :-      | :----------- |
| `remotes` | We use this only to be able to do `remotes::install_github()`. This enables us to install packages from GitHub that are not on CRAN yet. |
| `magrittr` | `magrittr` provides the `%>%` operator, which enables "piping" of data. Instead of doing `sample(letters, 3)` we can write `letters %>% sample(3)`. When chaining many function calls this may give more readable code. [Intro vignette](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) |
| `data.table` | This provides a more efficient and versatile replacement for the `data.frame` datatype built into R. [Intro vignette](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) |
| `ggplot2` | A very powerful plotting tool. [Overview with link to "cheat sheets"](https://ggplot2.tidyverse.org/) |
| `callr` | Encapsulating function calls in external R sessions. [GitHub page](https://github.com/r-lib/callr#readme) |
| `future` | Parallelization to make use of multicore functionality. [GitHub page](https://github.com/HenrikBengtsson/future) |
| `skimr` | Plotting data summaries for exploratory data analysis. [Vignette](https://cran.r-project.org/web/packages/skimr/vignettes/Using_skimr.html) |
| `DataExplorer` | Plotting data for exploratory data analysis. [Vignette](https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html) |
| `rpart.plot` | Plotting CART trees. [Website](http://www.milbo.org/rpart-plot/) |
| `precrec` | Plotting AUC curves. [Vignette](https://cran.r-project.org/web/packages/precrec/vignettes/introduction.html) |
| `OpenML` | [OpenML](https://www.openml.org/) is a (free, open-source) web platform providing machine learning datasets and problems. |
| `glmnet` | Provides the `"*.glmnet"` `Learner`s. Penalized regression is often surprisingly powerful, especially in high-dimensional settings. |
| `kknn` | Provides the `"*.kknn"` `Learner`s. k-nearest neighbour classification / regression is a classical machine learning technique. |
| `MASS` | Provides the `"*.lda"` and `"*.qda"` `Learner`s. |
| `ranger` | Provides the `"*.ranger"` `Learner`s. This is an implementation of the powerful "Random Forest" algorithm, which often works very well, even without parameter tuning. |
| `xgboost` | Provides the `"*.xgboost"` `Learner`s. Gradient boosting is often among the best performing machine learning methods, although it may require parameter tuning. |
| `e1071` | Provides the `"*.svm"` and `"classif.naive_bayes"` `Learner`s. SVMs (support vector machines) perform well, but are very dependent on correctly chosen kernel parameters. |